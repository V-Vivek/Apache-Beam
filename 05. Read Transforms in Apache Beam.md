## Read Transforms in Apache Beam

Apache Beam provides various read transforms to ingest data from different sources. Each read transform has specific parameters that control how data is read. Here are some common read transforms and their parameters:

### 1. ReadFromText

Reads data from a text file, where each line is treated as a separate element.

**Parameters**:
- `file_pattern`: (Mandatory) The file path or pattern to read from (e.g., `'input.txt'` or `'gs://bucket/*.txt'`).
- `skip_header_lines`: (Optional) Number of header lines to skip.
- `compression_type`: (Optional) Compression type of the file, such as `'AUTO'`, `'BZIP2'`, `'GZIP'`, `'UNCOMPRESSED'`, etc.
- `validate`: If set to TRUE it will verify the presence of file during pipeline creation. By default TRUE. If file is not present pipeline will not be created and beam will fail.
- `min_bundle_size`: (Optional) Specifies the minimum size of bundles that shouls be generated when splitting the source into bundles.

  `Note`: Beam internally divides a PCollection into multiple bundles and processes them parallely to achive distributed computing.

**Example**:
```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    lines = (pipeline
             | 'Read' >> beam.io.ReadFromText('input.txt', skip_header_lines=1))
```

### 2. ReadFromAvro
Reads data from Avro files.

**Parameters**:

- `file_pattern`: The file path or pattern to read from.
- `min_bundle_size`: (Optional) Specifies the minimum size of bundles that shouls be generated when splitting the source into bundles.
- `validate`: If set to TRUE it will verify the presence of file during pipeline creation. By default TRUE. If file is not present pipeline will not be created and beam will fail.
- `use_fastavro`: When set to true, uses `fastavro` library to read the avro file. This library is significantly faster & likely to become default in future.

**Example**:
```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    records = (pipeline
               | 'Read' >> beam.io.ReadFromAvro('input.avro'))
```

### 3. ReadFromParquet
Reads data from Parquet files.

**Parameters**:

- `file_pattern`: The file path or pattern to read from.
- `min_bundle_size`: (Optional) Specifies the minimum size of bundles that shouls be generated when splitting the source into bundles.
- `validate`: If set to TRUE it will verify the presence of file during pipeline creation. By default TRUE. If file is not present pipeline will not be created and beam will fail.
- `columns`: Specifies the list of columns that will be read from input file.

**Example**:
```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    rows = (pipeline
            | 'Read' >> beam.io.ReadFromParquet('input.parquet', columns=['column1', 'column2']))
```

### 4. ReadFromTFRecord
Reads data from TensorFlow records.
TFRecord format is a simple format for storing a sequence of binary records.

**Parameters**:

- `file_pattern`: The file path or pattern to read from.
- `min_bundle_size`: (Optional) Specifies the minimum size of bundles that shouls be generated when splitting the source into bundles.
- `validate`: If set to TRUE it will verify the presence of file during pipeline creation. By default TRUE. If file is not present pipeline will not be created and beam will fail.
- `coder`: Specifies the coder name to decode the input record. By default 'bytesCoder'

**Example**:
```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    rows = (pipeline
            | 'Read' >> ReadFromTFRecord('input.tfrecord', coder=coders.BytesCoder(), compression_type=CompressionTypes.DEFLATE, validate=True))
```

### 5. ReadFromBigQuery
Reads data from a BigQuery table or query.

**Parameters**:

- `table`: The BigQuery table to read from, specified as 'project:dataset.table' or DatasetReference.
- `query`: SQL query to execute.
- `use_standard_sql`: (Optional) Use Standard SQL dialect for queries.
- `flatten_results`: (Optional) Flatten nested and repeated fields in the query results.
- `selected_fields`: (Optional) List of fields to read from the table.

**Example**:
```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    rows = (pipeline
            | 'Read' >> beam.io.gcp.bigquery.ReadFromBigQuery(query='SELECT * FROM `project.dataset.table`', use_standard_sql=True))
```

### 6. ReadFromPubSub
Reads data from a Pub/Sub subscription or topic.

**Parameters**:

- `subscription`: The name of the subscription to read from.
- `topic`: The name of the topic to read from.
- `id_label`: (Optional) The label of the message id to use for deduplication. The value of this argument will be used as unique identifier.
- `with_attributes`: If set to TRUE then output elements will be of type 'objects', otherwise of type 'bytes'. FALSE by default.
- `timestamp_attribute`: (Optional) Specify the value to use for message timestamps. Specified argument should be a numerical value representing the number of milliseconds since the unix epoch.

**Note**: 
If you are passing `subscription` as argument keep `topic` as empty and vice versa. In short, you cannot pass both of these arguments at same time.

**Example**:

```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    messages = (pipeline
                | 'Read' >> beam.io.ReadFromPubSub(subscription='projects/project/subscriptions/subscription'))
```
