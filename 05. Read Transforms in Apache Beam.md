## Read Transforms in Apache Beam

Apache Beam provides various read transforms to ingest data from different sources. Each read transform has specific parameters that control how data is read. Here are some common read transforms and their parameters:

### 1. ReadFromText

Reads data from a text file, where each line is treated as a separate element.

**Parameters**:
- `file_pattern`: (Mandatory) The file path or pattern to read from (e.g., `'input.txt'` or `'gs://bucket/*.txt'`).
- `skip_header_lines`: (Optional) Number of header lines to skip.
- `compression_type`: (Optional) Compression type of the file, such as `'AUTO'`, `'BZIP2'`, `'GZIP'`, `'UNCOMPRESSED'`, etc.
- `validate`: If set to TRUE it will verify the presence of file during pipeline creation. By default TRUE. If file is not present pipeline will not be created and beam will fail.
- `min_bundle_size`: (Optional) Specifies the minimum size of bundles that shouls be generated when splitting the source into bundles.

  `Note`: Beam internally divides a PCollection into multiple bundles and processes them parallely to achive distributed computing.

**Example**:
```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    lines = (pipeline
             | 'Read' >> beam.io.ReadFromText('input.txt', skip_header_lines=1))
```

### 2. ReadFromAvro
Reads data from Avro files.

**Parameters**:

- `file_pattern`: The file path or pattern to read from.
- `validate`: (Optional) Flag to verify that the file pattern matches at least one file.
- `coder`: (Optional) A coder to use when reading data.

**Example**:
```python
import apache_beam as beam

with beam.Pipeline() as pipeline:
    records = (pipeline
               | 'Read' >> beam.io.ReadFromAvro('input.avro'))
```
