## Write Transforms in Apache Beam

Apache Beam provides several write transforms to output data to various sinks. Each transform is designed to handle different data formats and storage systems, allowing flexibility in data output options.

### 1. WriteToText

Writes data to a text file, where each element in the PCollection is written as a single line.

**Parameters**:
- `file_path_prefix`: (Required) The file path prefix for the output files. Beam appends unique suffixes to each file.
- `file_name_suffix`: (Optional) The file name suffix to use for the output files (e.g., `.txt`).
- `append_trailing_newlines`: (Optional) Whether to append a newline to each element (default is `True`).
- `num_shards`: (Optional) The number of output shards (files) to write. If not set, Beam determines the number automatically. It is recommend to not set this so that beam can dynamically decide it based on size of data.
- `shard_name_template`: (Optional) A template string for naming output files.
- `compression_type`: (Optional) The compression type for the output files, such as `UNCOMPRESSED`, `GZIP`, or `BZIP2`.
- `header`: Specifies the Header line of the Output file.

**Example**:
```python
import apache_beam as beam

def run():
    with beam.Pipeline() as pipeline:
        data = ['apple', 'banana', 'cherry']
        pcollection = pipeline | 'Create' >> beam.Create(data)
        pcollection | 'WriteToText' >> beam.io.WriteToText('output', file_name_suffix='.txt')

if __name__ == '__main__':
    run()
# Output file -> output-00000-of-00001.txt
# prefix -> output
# shard identifier -> 00000-of-00001
# suffix -> .txt
```

### 2. WriteToAvro
Writes data to Avro files.

**Parameters**:

- `file_path_prefix`: (Required) The file path prefix for the output files.
- `schema`: (Required) The Avro schema to use for encoding the data.
- `file_name_suffix`: (Optional) The file name suffix for the output files (default is .avro).
- `codec`: (Optional) The codec to use for compression (default is deflate).
- `num_shards`: (Optional) The number of output shards (files) to write.
- `use_fastavro`: When set to TRUE, uses the 'fastavro' library to write the Avro file.

**Example**:
```python
import apache_beam as beam
from apache_beam.io.avroio import WriteToAvro

def run():
    with beam.Pipeline() as pipeline:
        schema = {
            "type": "record",
            "name": "Fruit",
            "fields": [
                {"name": "name", "type": "string"},
                {"name": "quantity", "type": "int"}
            ]
        }
        data = [{'name': 'apple', 'quantity': 10}, {'name': 'banana', 'quantity': 20}]
        pcollection = pipeline | 'Create' >> beam.Create(data)
        pcollection | 'WriteToAvro' >> WriteToAvro('output', schema=schema)

if __name__ == '__main__':
    run()
```

### 3. WriteToParquet
Writes data to Parquet files.

**Parameters**:

- `file_path_prefix`: (Required) The file path prefix for the output files.
- `schema`: (Required) The Parquet schema to use for encoding the data.
- `file_name_suffix`: (Optional) The file name suffix for the output files (default is .parquet).
- `codec`: (Optional) The codec to use for compression (default is snappy).
- `num_shards`: (Optional) The number of output shards (files) to write.
- `row_group_buffer_size`: Specifies the byte size of the row group buffer. Default '67108864'
- `record_batch_size`: Specifies the number of records in each record batch. Default '1000'

**Example**:
```python
import apache_beam as beam
from apache_beam.io.parquetio import WriteToParquet
import pyarrow as pa

def run():
    with beam.Pipeline() as pipeline:
        schema = pa.schema([
            ('name', pa.string()),
            ('quantity', pa.int32())
        ])
        data = [{'name': 'apple', 'quantity': 10}, {'name': 'banana', 'quantity': 20}]
        pcollection = pipeline | 'Create' >> beam.Create(data)
        pcollection | 'WriteToParquet' >> WriteToParquet('output', schema=schema)

if __name__ == '__main__':
    run()
```

### 4. WriteToBigQuery
Writes data to a BigQuery table.

**Parameters**:

- `table`: (Required) The BigQuery table to write to, specified as 'project:dataset.table'.
- `schema`: (Optional) The schema to use if the table does not exist.
- `create_disposition`: (Optional) Specifies whether to create the table if it does not exist (default is CREATE_IF_NEEDED).
- `write_disposition`: (Optional) Specifies the action to take if the table already exists (default is WRITE_APPEND).
- `custom_gcs_temp_location`: (Optional) GCS path for temporary files.

**Example**:
```python
import apache_beam as beam

def run():
    with beam.Pipeline() as pipeline:
        schema = 'name:STRING,quantity:INTEGER'
        data = [{'name': 'apple', 'quantity': 10}, {'name': 'banana', 'quantity': 20}]
        pcollection = pipeline | 'Create' >> beam.Create(data)
        pcollection | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(
            'project:dataset.table',
            schema=schema,
            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
        )

if __name__ == '__main__':
    run()
```

### 5. WriteToPubSub
Writes data to a Pub/Sub topic.

**Parameters**:

- `topic`: (Required) The name of the Pub/Sub topic to write to (e.g., 'projects/project/topics/topic').
- `with_attributes`: (Optional) Whether to include attributes with each Pub/Sub message. If set to TRUE then input elements will be of type 'Objects', otherwise of type 'Bytes'. By default FALSE.
- `id_label`: If set, will set an attribute for each Cloud Pub/Sub message with the given name and a unique value.
- `timestamp_attribute`: Will set an attribute for each Cloud Pub/Sub message with the given name and the message's publish time as value.

**Example**:
```python
import apache_beam as beam

def run():
    with beam.Pipeline() as pipeline:
        data = ['message1', 'message2', 'message3']
        pcollection = pipeline | 'Create' >> beam.Create(data)
        pcollection | 'WriteToPubSub' >> beam.io.WriteToPubSub('projects/project/topics/topic')

if __name__ == '__main__':
    run()
```
